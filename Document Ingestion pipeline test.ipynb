{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94bce1e-fec6-4836-bc66-e570975479ee",
   "metadata": {},
   "source": [
    "### Package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3c056-f665-44b7-b9a6-1a988a0be856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "282648fb-8f35-4dd7-8e8a-f9fec4960ca0",
   "metadata": {},
   "source": [
    "## Document Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dacf00-2959-4f22-9fee-0dfc92fcf67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ID of SME for data labeling\n",
    "\n",
    "sme_id = 'sme_001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f79931-4d5f-4cad-bd1a-ecb44388fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add9337-d04b-4e3e-89a8-7667b600f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be93a5-abda-4b8d-90f7-9ccad873da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, get_response_synthesizer, StorageContext\n",
    "from llama_index.core import DocumentSummaryIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf17a8e-01bb-4083-8053-24310cdc8ed7",
   "metadata": {},
   "source": [
    "#### Parsing files and loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e0bc8-e0ed-456a-9799-cfcb4a1ea7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./ritika_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024091e-b425-45b4-990f-51490caa9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ingesting documents from Local\n",
    "\n",
    "filenames = [\"Decarbonizing-the-Built-World-A-Call-to-Action-2023-03-07\",\n",
    "             \"Digital-Twin-Capabilities-Periodic-Table-User-Guide\",\n",
    "             \"Digital-Twin-System-Interoperability-Framework-12072021\",\n",
    "             \"DTC-Reality-Capture-Industry-User-Guide-for-Tenant-Improvement-Projects-2023-06-07\",\n",
    "             \"Infrastructure-Digital-Twin-Maturity-Model\"]\n",
    "             #\"Platform-Stack-Architectural-Framework\",\n",
    "             #\"Reality-Capture-A-Digital-Twin-Foundation\",\n",
    "             #\"SMM-Digital-Twin-Profile-2022-06-20\",\n",
    "             #\"User-Guide-1-Why-and-What-2023-07-18\",\n",
    "             #\"User-Guide-2-Identifying-and-Aligning\",\n",
    "             #\"User-Guide-3_A-Whole_Systems_Approach\"]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for filename in filenames:\n",
    "    doc = SimpleDirectoryReader(input_files=[f\"{data_root}/{filename}.pdf\"]).load_data()\n",
    "    doc[0].doc_id = filename.replace(\".pdf\",\"\")\n",
    "    docs.extend(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddb613-a2a4-479d-b24b-90c02b0a6d45",
   "metadata": {},
   "source": [
    "#### Redis-Based Ingestion Pipeline for Doc store and Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621d366-f4c4-432c-8a49-436e3924c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache\n",
    "from llama_index.storage.docstore.redis import RedisDocumentStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.redis import RedisVectorStore\n",
    "from llama_index.storage.index_store.redis import RedisIndexStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84446d87-731a-45ee-915c-fc227b6c8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "text_splitter = SentenceSplitter(chunk_size=3000)\n",
    "embed_model = OpenAI(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277188e2-f618-4c2c-98c3-56bbc0257d2c",
   "metadata": {},
   "source": [
    "#### Setting up document store, index store and vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d07905-08da-4079-b0d6-c3502902c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up connection to Redis two ways:\n",
    "\n",
    "## Through Python Redis client:\n",
    "\n",
    "import redis\n",
    "redis_client = redis.Redis(\n",
    "  host='',\n",
    "  port=,\n",
    "  password='')\n",
    "\n",
    "## Through host and port:\n",
    "\n",
    "#REDIS_HOST = os.getenv(\"REDIS_HOST\", \"127.0.0.1\")\n",
    "#REDIS_PORT = os.getenv(\"REDIS_PORT\", 6379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3fd123-66a5-461f-afcb-3c4acc4b33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining PGVector database:\n",
    "\n",
    "from sqlalchemy import make_url\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "url = make_url(\"postgres://postgres.<username>:<password>@aws-0-us-west-1.pooler.supabase.com:5432/postgres\")\n",
    "db_name=\"postgres\"\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=url.database,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=sme_id,\n",
    "    embed_dim=1536,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28effa4-7db8-4767-a47b-ba6c22aa20ef",
   "metadata": {},
   "source": [
    "#### Creating storage context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826796c3-9999-48b1-abed-569793361c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Through Redis client:\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=RedisDocumentStore.from_redis_client(\n",
    "        redis_client=redis_client, namespace=sme_id\n",
    "    ),\n",
    "    \n",
    "    index_store=RedisIndexStore.from_redis_client(\n",
    "        redis_client=redis_client, namespace=sme_id\n",
    "    ),\n",
    "    vector_store = vector_store\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode = \"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f4325-3b67-4eb6-8666-4870fffd7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Through Redis host and port:\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=RedisDocumentStore.from_redis_client(\n",
    "        host=REDIS_HOST, port=REDIS_PORT, namespace=\"llama_index\"\n",
    "    ),\n",
    "    \n",
    "    index_store=RedisIndexStore.from_redis_client(\n",
    "        host=REDIS_HOST, port=REDIS_PORT, namespace=\"llama_index\"\n",
    "    ),\n",
    "    vector_store = vector_store\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode = \"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac557b-9f4a-4e4c-80b3-713ca6d56afa",
   "metadata": {},
   "source": [
    "#### Creating Document Summary Index and storing in respective db locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66879187-c811-4936-b771-4677c277473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_index = DocumentSummaryIndex.from_documents(docs,\n",
    "    llm=llm,\n",
    "    transformations=[text_splitter],\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    "    storage_context = storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d755e1-814c-4f1b-99c3-633e336ce81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fin ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea697738-5148-415e-a26f-fe7ae8b6d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Restart kernel to test Pipeline below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8470c09-5771-45ab-91ab-5514292dc30d",
   "metadata": {},
   "source": [
    "## Document Retreival Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87132b68-6623-4494-8afb-231904c22f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ID of SME for data labeling\n",
    "\n",
    "sme_id = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef3e4c5-25fd-45f5-be81-dc8a5cb837af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294931b7-a80c-4ada-9955-2910ba74b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c57a577-f01e-4fa5-98ce-9227fcaeb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, get_response_synthesizer, StorageContext\n",
    "from llama_index.core import DocumentSummaryIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c88b4d-6aac-4208-b4e3-fbdf6e6c0778",
   "metadata": {},
   "source": [
    "#### Recreating storage context object using same DB connections from ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f22e74-f4f8-4be7-83dc-27efe19fbe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache\n",
    "from llama_index.storage.docstore.redis import RedisDocumentStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.redis import RedisVectorStore\n",
    "from llama_index.storage.index_store.redis import RedisIndexStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02e3c80-dc51-42e0-a657-336f98f112b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "text_splitter = SentenceSplitter(chunk_size=3000)\n",
    "embed_model = OpenAI(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807ce53d-f5fa-492a-ad4d-dee5d4215341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connecting to Redis DB through Python Redis Client (Refer to ingestion pipeline above for Redis Host-Port access)\n",
    "\n",
    "import redis\n",
    "redis_client = redis.Redis(\n",
    "  host='',\n",
    "  port=,\n",
    "  password='')\n",
    "\n",
    "## Setting up PGVector database\n",
    "\n",
    "from sqlalchemy import make_url\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "url = make_url(\"postgres://postgres.<username>:<password>@aws-0-us-west-1.pooler.supabase.com:5432/postgres\")\n",
    "db_name=\"postgres\"\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=url.database,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=sme_id,\n",
    "    embed_dim=1536,  # openai embedding dimension\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7209be-079e-48c0-a76e-30b169d75597",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-creating Storage context through Redis client (Refer to ingestion pipeline above for Redis Host-Port access):\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=RedisDocumentStore.from_redis_client(\n",
    "        redis_client=redis_client, namespace=sme_id\n",
    "    ),\n",
    "    index_store=RedisIndexStore.from_redis_client(\n",
    "        redis_client=redis_client, namespace=sme_id\n",
    "    ),\n",
    "    vector_store = vector_store\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode = \"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494b0b4-53d4-4f09-86df-e9ae019a0d6b",
   "metadata": {},
   "source": [
    "#### Creating steps to final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d16ff0-a2ab-40eb-9fe6-51a6be717ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating prompt template\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "new_summary_tmpl_str = (\n",
    "    \"Context information is below:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge,\"\n",
    "    \"answer the query in the style of a McKinsey, Bain or BCG consultant who specializes in digital transformation strategies.\"\n",
    "    \"Your goal is to help business users succeed in their digital transformation journeys.\"\n",
    "    \"Do not use any context outside of these documents.\"\n",
    "    \"If a question is outside of your area of expertise, politely refuse to answer and suggest alternative topics of discussion from the context provided.\"\n",
    "    \"You should maintain a friendly yet professional tone.\"\n",
    "    \"Use detailed bullet points whenever relevant.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1d4c6-58a2-4a95-bf37-072c0eebd6c7",
   "metadata": {},
   "source": [
    "#### Loading document summary index using storage context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f51d093-5649-4237-b37c-bf2edbfd0225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information from multiple sources is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the information from multiple sources and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below:\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge,answer the query in the style of a McKinsey, Bain or BCG consultant who specializes in digital transformation strategies.Your goal is to help business users succeed in their digital transformation journeys.Do not use any context outside of these documents.If a question is outside of your area of expertise, politely refuse to answer and suggest alternative topics of discussion from the context provided.You should maintain a friendly yet professional tone.Use detailed bullet points whenever relevant.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.document_summary import (\n",
    "    DocumentSummaryIndexEmbeddingRetriever,\n",
    ")\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "doc_summary_index = load_index_from_storage(\n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "# Configuring response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(streaming=True, response_mode=\"tree_summarize\")\n",
    "\n",
    "## Creating Retreiver object\n",
    "retriever = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    doc_summary_index,\n",
    "    choice_batch_size=10,\n",
    "    choice_top_k=5\n",
    ")\n",
    "\n",
    "# Assembling query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "## Checking default query prompt:\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)\n",
    "\n",
    "## Modifying query prompt\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": new_summary_tmpl}\n",
    ")\n",
    "\n",
    "## Checking modified query prompt:\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc5c45-a2f5-4e0b-b6e0-726f8702d3f3",
   "metadata": {},
   "source": [
    "### Implementing embeddings Retirever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "353d6f9a-7d54-492f-a02e-6604ea7e76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When implementing a digital twin, it is crucial to consider the following key factors:\n",
      "\n",
      "1. Holistic Approach: View the digital twin as a holistic system of systems that are interconnected within a common digital thread. This ensures that all components work together seamlessly to provide maximum value.\n",
      "\n",
      "2. Specific Use Cases: Connect the digital twin to solving specific use cases relevant to your business. Without this connection, the digital twin may lack the ability to provide tangible value.\n",
      "\n",
      "3. Data Communication and Collaboration: Ensure that the digital twin facilitates effective communication, collaboration, and correlation of data in a meaningful way. This will enable stakeholders to make informed decisions based on real-time insights.\n",
      "\n",
      "4. Robust Tool Development: Develop a robust digital twin tool that adds value throughout the entire digital building lifecycle. This includes considering factors such as data accuracy, scalability, and integration with existing systems.\n",
      "\n",
      "5. Stakeholder Alignment: Identify and align stakeholders from various departments and levels within the organization to ensure buy-in and support for the digital twin implementation.\n",
      "\n",
      "6. Continuous Improvement: Implement a process for continuous improvement and optimization of the digital twin based on feedback from users and evolving business needs.\n",
      "\n",
      "By considering these important factors, businesses can successfully implement a digital twin that drives value and enhances their digital transformation journey.--- 5.755197048187256 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# query\n",
    "response = query_engine.query(\"What are the most important considerations when implementing a digital twin?\")\n",
    "response.print_response_stream()\n",
    "\n",
    "## Use streaming response in block:\n",
    "'''for text in streaming_response.response_gen:\n",
    "    # do something with text as they arrive.\n",
    "    pass\n",
    "'''\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8f57b-d88f-4258-bdba-a6627ea0cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fin ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6bde9b-b333-4961-a432-c37859d5fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Restart kernel to test Pipeline below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c2cc92-c8f6-4990-9b3e-d46dc9cc6305",
   "metadata": {},
   "source": [
    "## SME Data Insertion-Deletion (updation) pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea67cc0-5431-4fe7-9732-35b5ddc3b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ID of SME for data labeling\n",
    "\n",
    "sme_id = 'sme_001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8614e-23e8-4742-a66a-c0323547da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ffabe-6ddd-42fb-a666-d0574238bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f2df2-3307-4f08-a473-26773a7ca574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, get_response_synthesizer, StorageContext\n",
    "from llama_index.core import DocumentSummaryIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97b598-f215-4f18-96fb-ee956b6d6651",
   "metadata": {},
   "source": [
    "#### Parsing new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c8a54-84cc-4be9-b5da-b0894ece3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./ritika_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e7c71-bfdb-41f5-a1cf-18b4d81e4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [#\"Decarbonizing-the-Built-World-A-Call-to-Action-2023-03-07\",\n",
    "             #\"Digital-Twin-Capabilities-Periodic-Table-User-Guide\",\n",
    "             #\"Digital-Twin-System-Interoperability-Framework-12072021\",\n",
    "             #\"DTC-Reality-Capture-Industry-User-Guide-for-Tenant-Improvement-Projects-2023-06-07\",\n",
    "             #\"Infrastructure-Digital-Twin-Maturity-Model\"]\n",
    "             \"Platform-Stack-Architectural-Framework\",\n",
    "             \"Reality-Capture-A-Digital-Twin-Foundation\",\n",
    "             \"SMM-Digital-Twin-Profile-2022-06-20\",\n",
    "             \"User-Guide-1-Why-and-What-2023-07-18\",\n",
    "             \"User-Guide-2-Identifying-and-Aligning\",\n",
    "             \"User-Guide-3_A-Whole_Systems_Approach\"]\n",
    "\n",
    "new_docs = []\n",
    "\n",
    "for filename in filenames:\n",
    "    doc = SimpleDirectoryReader(input_files=[f\"{data_root}/{filename}.pdf\"]).load_data()\n",
    "    doc[0].doc_id = filename.replace(\".pdf\",\"\")\n",
    "    new_docs.extend(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ade2f-61de-4c8c-bb35-4ca82d90664a",
   "metadata": {},
   "source": [
    "#### Redis-Based Ingestion Pipeline for Doc store and Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72968092-a7e7-43c9-a612-3eec4775adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache\n",
    "from llama_index.storage.docstore.redis import RedisDocumentStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.redis import RedisVectorStore\n",
    "from llama_index.storage.index_store.redis import RedisIndexStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0567b6-4256-4365-93cb-2d7d3373a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "text_splitter = SentenceSplitter(chunk_size=3000)\n",
    "embed_model = OpenAI(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e6463-07c7-4cb2-ae02-302a64ea00e8",
   "metadata": {},
   "source": [
    "#### Setting up document store, index store and vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad40053-8ed1-4771-8fa5-404075f8c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up connection to Redis two ways:\n",
    "\n",
    "## Through Python Redis client:\n",
    "\n",
    "import redis\n",
    "redis_client = redis.Redis(\n",
    "  host='',\n",
    "  port=,\n",
    "  password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc5531-b311-47e9-b237-6a584c8dcb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining PGVector database:\n",
    "\n",
    "from sqlalchemy import make_url\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "url = make_url(\"postgres://postgres.<username>:<password>@aws-0-us-west-1.pooler.supabase.com:5432/postgres\")\n",
    "db_name=\"postgres\"\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=url.database,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=sme_id,\n",
    "    embed_dim=1536,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbc5d5-4d4d-4ff4-8343-033b86771c55",
   "metadata": {},
   "source": [
    "#### Loading Index from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12831a82-53b6-47e9-88bc-0dff4916f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-creating Storage context through Redis client (Refer to ingestion pipeline above for Redis Host-Port access):\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=RedisDocumentStore.from_redis_client(\n",
    "        redis_client=redis_client, namespace=sme_id\n",
    "    ),\n",
    "    index_store=RedisIndexStore.from_redis_client(\n",
    "        redis_client=redis_client, namespace=sme_id\n",
    "    ),\n",
    "    vector_store = vector_store\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode = \"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2504a-8ad7-4c12-89ad-997e14f2b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "doc_summary_index = load_index_from_storage(\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae65e0-f646-485a-ad44-0670db13d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_index.refresh_ref_docs(new_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1518b7-4fca-45e5-a757-65d15251caff",
   "metadata": {},
   "source": [
    "### Checking response after adding new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e45bc-9de0-4a21-a8e3-de4aca5c60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating prompt template\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "new_summary_tmpl_str = (\n",
    "    \"Context information is below:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge,\"\n",
    "    \"answer the query in the style of a McKinsey, Bain or BCG consultant who specializes in digital transformation strategies.\"\n",
    "    \"Your goal is to help business users succeed in their digital transformation journeys.\"\n",
    "    \"Do not use any context outside of these documents.\"\n",
    "    \"If a question is outside of your area of expertise, politely refuse to answer and suggest alternative topics of discussion from the context provided.\"\n",
    "    \"You should maintain a friendly yet professional tone.\"\n",
    "    \"Use detailed bullet points whenever relevant.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a747f-f300-4098-8b44-99a0a78c10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.document_summary import (\n",
    "    DocumentSummaryIndexEmbeddingRetriever,\n",
    ")\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "#doc_summary_index = load_index_from_storage(\n",
    "#    storage_context=storage_context\n",
    "#)\n",
    "\n",
    "# Configuring response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(streaming=True, response_mode=\"tree_summarize\")\n",
    "\n",
    "## Creating Retreiver object\n",
    "retriever = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    doc_summary_index,\n",
    "    choice_batch_size=10,\n",
    "    choice_top_k=5\n",
    ")\n",
    "\n",
    "# Assembling query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "## Checking default query prompt:\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)\n",
    "\n",
    "## Modifying query prompt\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": new_summary_tmpl}\n",
    ")\n",
    "\n",
    "## Checking modified query prompt:\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13781788-ee0c-4f99-8207-264492c26baa",
   "metadata": {},
   "source": [
    "### Implementing embeddings Retirever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59d9c6-57b1-4d37-881c-5d62c902d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# query\n",
    "response = query_engine.query(\"How can companies think about setting up their digital twins?\")\n",
    "response.print_response_stream()\n",
    "\n",
    "## Use streaming response in block:\n",
    "'''for text in streaming_response.response_gen:\n",
    "    # do something with text as they arrive.\n",
    "    pass\n",
    "'''\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134a7c1-1a26-46be-882d-ddf710d1c6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
